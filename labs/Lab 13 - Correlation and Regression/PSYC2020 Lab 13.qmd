---
title: "Correlation and Regression"
subtitle: "PSYC 2020-A01 / PSYC 6022-A01 | 2025-11-14 | Lab 13"
author: "Jessica Helmer"
format: 
  revealjs:
    slide-number: true
    show-slide-number: all
    css: PSYC2020L_styles.css
engine: knitr
webr:
  packages:
    - rio
    - datasets
    - palmerpenguins
execute:
  echo: true
width: 1200
---

## Outline

* Assignment 12 Review
* Correlation
* Regression

Learning objectives:

**R:** Correlation, regression

```{r}
#| echo: false
library(tidyverse)
```


## Assignment 12 Review

[placeholder for Assignment 12 review]



# Correlation


## Correlation

Measure of association: how strongly are two variables related?

Indexes the linear relationship between two variables

We will cover correlation for two continuous variables


## Correlation

Only measures **linear** relationships

::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 4
#| fig-align: center
#| layout-ncol: 4
n <- 100

d1 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = x + rnorm(n, 0, 1))
d2 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = runif(n, 2, 8) + rnorm(n, 0, 1))
d3 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = .5* (x - 5)^2  + rnorm(n, 0, .1))
d4 <- data.frame(x = runif(n, 1, 9) + rnorm(n, 0, .5)) |>
  mutate(y = 2.5 * sin(2 * x) + 5 + rnorm(n, 0, .2))

sctplt <- function(df) {
  r <- cor(df$x, df$y)
  
  df |>
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    scale_x_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    scale_y_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) +
    guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
    labs(title = paste("*r* =", round(r, 2))) +
    theme_classic(base_size = 16) +
    theme(aspect.ratio = 1,
          plot.title = ggtext::element_markdown(hjust = .5))
}

sctplt(d1)
sctplt(d2)
sctplt(d3)
sctplt(d4)
```

### Code
```{r}
#| eval: false
n <- 100

d1 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = x + rnorm(n, 0, 1))
d2 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = runif(n, 2, 8) + rnorm(n, 0, 1))
d3 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = .5* (x - 5)^2  + rnorm(n, 0, .1))
d4 <- data.frame(x = runif(n, 1, 9) + rnorm(n, 0, .5)) |>
  mutate(y = 2.5 * sin(2 * x) + 5 + rnorm(n, 0, .2))

sctplt <- function(df) {
  r <- cor(df$x, df$y)
  
  df |>
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    scale_x_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    scale_y_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) +
    guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
    labs(title = paste("*r* =", round(r, 2))) +
    theme_classic(base_size = 16) +
    theme(aspect.ratio = 1,
          plot.title = ggtext::element_markdown(hjust = .5))
}

sctplt(d1)
sctplt(d2)
sctplt(d3)
sctplt(d4)
```
:::


## Correlation Generally

$$ r_{xy} = \frac{\sum_{i=1}^{I} (x_i - \bar{x}) (y_i - \bar{y})}{(n-1)s_x s_y} $$

where 

[$i$ = index of observation $i$ out of $I$ total observations]{.subpoint}

[$\bar{x}$ = mean of $x$]{.subpoint}

[$\bar{y}$ = mean of $y$]{.subpoint}

[$s_x$ = standard deviation of $x$]{.subpoint}

[$s_y$ = standard deviation of $y$]{.subpoint}


## Correlation Generally

$$ r_{xy} = \frac{\sum_{i=1}^{I} (x_i - \bar{x}) (y_i - \bar{y})}{(n-1)s_x s_y} $$

By subtracting the mean and dividing by the standard deviation, this formula is converting the observations to *z*-scores.


## Correlation Example

$$ r_{xy} = \frac{\sum_{i=1}^{I} (x_i - \bar{x}) (y_i - \bar{y})}{(n-1)s_x s_y} $$

Let's find the correlation between iris' sepal length and sepal width. For demonstration, let's only use the first four rows.

:::: {.columns}
::: {.column width="50%"}
```{r}
iris_small <- iris |>
  select(Sepal.Length, Sepal.Width) |>
  head(4)
iris_small
```
:::

::: {.column width="50%"}
```{r}
x_bar <- mean(iris_small$Sepal.Length)
x_sd <- sd(iris_small$Sepal.Length)

y_bar <- mean(iris_small$Sepal.Width)
y_sd <- sd(iris_small$Sepal.Width)

paste(x_bar, x_sd, y_bar, y_sd, sep = ", ")
```
:::
::::


## Correlation Example

$$ r_{xy} = \frac{\sum_{i=1}^{I} (x_i - \bar{x}) (y_i - \bar{y})}{(n-1)s_x s_y} $$

Let's find the correlation between iris' sepal length and sepal width. For demonstration, let's only use the first four rows.

:::: {.columns}
::: {.column width="58%"}
```{r}
#| code-line-numbers: false
iris_small <- iris_small |>
  mutate(sq_diff_x = Sepal.Length - x_bar,
         sq_diff_y = Sepal.Width - y_bar,
         product = sq_diff_x * sq_diff_y)
iris_small
```
:::

::: {.column width="42%"}
```{r}
#| code-line-numbers: false
sum(iris_small$product) /
  ((nrow(iris_small) - 1) * x_sd * y_sd)
```
:::
::::


## Correlation in R

$$ r_{xy} = \frac{\sum_{i=1}^{I} (x_i - \bar{x}) (y_i - \bar{y})}{(n-1)s_x s_y} $$

Let's find the correlation between iris' sepal length and sepal width. For demonstration, let's only use the first four rows.

:::: {.columns}
::: {.column width="50%"}
```{r}
iris_small <- iris |>
  select(Sepal.Length, Sepal.Width) |>
  head(4)
iris_small
```
:::

::: {.column width="50%"}
```{r}
cor(iris_small$Sepal.Length, 
    iris_small$Sepal.Width)
```
:::
::::


## Correlation in R

As a note, those four points were not actually representative of the general trend!

:::: {.columns}
::: {.column width="50%"}
::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
iris |>
  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point(alpha = .5, shape = 16) +
  geom_point(data = head(iris, 4), fill = "#FCACA3", color = "salmon", size = 3.3, shape = 23) +
  scale_x_continuous(breaks = c(4, 6, 8), labels = c(4, 6, 8)) +
  scale_y_continuous(breaks = c(1, 3, 5), labels = c(1, 3, 5)) +
  coord_cartesian(xlim = c(4, 8), ylim = c(1, 5)) +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
```

### Code
```{r}
#| eval: false
iris |>
  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point(alpha = .5, shape = 16) +
  geom_point(data = head(iris, 4), fill = "#FCACA3", color = "salmon", size = 3.3, shape = 23) +
  scale_x_continuous(breaks = c(4, 6, 8), labels = c(4, 6, 8)) +
  scale_y_continuous(breaks = c(1, 3, 5), labels = c(1, 3, 5)) +
  coord_cartesian(xlim = c(4, 8), ylim = c(1, 5)) +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
```
:::
:::

::: {.column width="50%"}
```{r}
cor(iris$Sepal.Length, iris$Sepal.Width)
```
:::
::::



# Regression


## Regression

If you are working with just one variable, what is the best way you can represent the data?

If you had to pick just one statistic, what might you choose?

::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 3
#| fig-align: center
#| layout-ncol: 3
n <- 20

d1 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = runif(n, 2, 8) + 2 + rnorm(n, 0, 1))
d2 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = runif(n, 2, 8) + rnorm(n, 0, 1))
d3 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = runif(n, 2, 8) + -2 + rnorm(n, 0, 1))

sctplt_mean <- function(df) {
  df |>
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) +
    guides(y = guide_axis(cap = "both"), x = "none") +
    labs(x = NULL) +
    theme_classic(base_size = 16) +
    theme(aspect.ratio = 1)
}

sctplt_mean(d1)
sctplt_mean(d2)
sctplt_mean(d3)
```

### Code
```{r}
#| eval: false
#| layout-ncol: 4
n <- 20

d1 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = runif(n, 2, 8) + 2 + rnorm(n, 0, 1))
d2 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = runif(n, 2, 8) + rnorm(n, 0, 1))
d3 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = runif(n, 2, 8) + -2 + rnorm(n, 0, 1))

sctplt_mean <- function(df) {
  df |>
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) +
    guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
    labs(x = NULL) +
    theme_classic(base_size = 16) +
    theme(aspect.ratio = 1)
}

sctplt_mean(d1)
sctplt_mean(d2)
sctplt_mean(d3)
```
:::


## Regression

If we consider "best" to mean *minimizing the squared distances from the data*, the **mean** is the best fit.

The mean has the smallest squared errors from the observations.

::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 3
#| fig-align: center
#| layout-ncol: 3

sctplt_mean(d1) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_point()
sctplt_mean(d2) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_point()
sctplt_mean(d3) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_point()
```

### Code
```{r}
#| eval: false
sctplt_mean(d1) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon", linetype = "longdash")
sctplt_mean(d2) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon", linetype = "longdash")
sctplt_mean(d3) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon", linetype = "longdash")
```
:::


## Regression

If we consider "best" to mean *minimizing the squared distances from the data*, the **mean** is the best fit.

The mean has the smallest squared errors from the observations.

::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 3
#| fig-align: center
#| layout-ncol: 3

sctplt_mean(d1) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
sctplt_mean(d2) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
sctplt_mean(d3) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
```

### Code
```{r}
#| eval: false
#| layout-ncol: 4
sctplt_mean(d1) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
sctplt_mean(d2) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
sctplt_mean(d3) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
```
:::


## Regression

This is equivalent to the "mean intercept model" regression equation

$$ y_i = \beta_0 + \epsilon_i $$

::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 3
#| fig-height: 3
#| fig-align: center
#| layout-ncol: 3

sctplt_mean(d1) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
sctplt_mean(d2) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
sctplt_mean(d3) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
```

### Code
```{r}
#| eval: false
#| layout-ncol: 4
sctplt_mean(d1) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
sctplt_mean(d2) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
sctplt_mean(d3) +
  geom_hline(aes(yintercept = mean(y)), color = "salmon") +
  geom_segment(aes(y = y, yend = mean(y)), color = "salmon", linetype = "dashed") +
  geom_point()
```
:::


## Intercept Model to Regression

When we have *one* variable of interest, the mean is the best single estimator. 

[○ $y = \beta_0 = \bar{y}$ would be the line of best fit]{.subpoint}

If we have *two* variables and we want to predict one from the other, we now may need a *slope* to find a line of best fit.

[○ This model will typically have both an intercept and a slope]{.subpoint}


## Intercept Model to Regression

We have the same question: which line can represent points most effectively?

:::: {.columns}
::: {.column width="50%"}
::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
n <- 30

d1 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = x + rnorm(n, 0, 1))

sctplt_reg <- function(df) {
  df |>
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    scale_x_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    scale_y_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) +
    guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
    theme_classic(base_size = 16) +
    theme(aspect.ratio = 1,
          plot.title = ggtext::element_markdown(hjust = .5))
}

sctplt_reg(d1)
```

### Code
```{r}
#| eval: false
n <- 30

d1 <- data.frame(x = runif(n, 2, 8) + rnorm(n, 0, 1)) |>
  mutate(y = 5 + 0.2 * x + rnorm(n, 0, 1))

sctplt_reg <- function(df) {
  df |>
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    scale_x_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    scale_y_continuous(breaks = c(0, 5, 10), labels = c(0, 5, 10)) +
    coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) +
    guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
    theme_classic(base_size = 16) +
    theme(aspect.ratio = 1,
          plot.title = ggtext::element_markdown(hjust = .5))
}

sctplt_reg(d1)
```
:::
:::

::: {.column .fragment width="50%"}
Lots of potential lines we could use
:::
::::


## Intercept Model to Regression

We have the same question: which line can represent points most effectively?

:::: {.columns}
::: {.column width="50%"}
::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
sctplt_reg(d1)  +
  geom_segment(aes(x = 0, xend = 10, y = mean(x)), color = "salmon") +
  geom_segment(aes(x = 0, xend = 10, y = 0, yend = 10), color = "salmon") +
  geom_segment(aes(x = 0, xend = 10, y = 2, yend = 8), color = "salmon") +
  geom_segment(aes(x = 0, xend = 10, y = -2, yend = 12), color = "salmon") +
  geom_point()
```

### Code
```{r}
#| eval: false
sctplt_reg(d1)  +
  geom_segment(aes(x = 0, xend = 10, y = mean(x)), color = "salmon") +
  geom_segment(aes(x = 0, xend = 10, y = 0, yend = 10), color = "salmon") +
  geom_segment(aes(x = 0, xend = 10, y = 2, yend = 8), color = "salmon") +
  geom_segment(aes(x = 0, xend = 10, y = -2, yend = 12), color = "salmon") +
  geom_point()
```
:::
:::

::: {.column width="50%"}
Lots of potential lines we could use
:::
::::


## Intercept Model to Regression

Considering the relationship of $y$ with another variable $x$

$$ y_i = \beta_0 + \beta_1 \times x_i + \epsilon_i $$

where 

[$i$ = index of observation $i$ out of $I$ total observations]{.subpoint}

[$y$ = outcome]{.subpoint}

[$x$ = predictor]{.subpoint}

[$\beta_0$ = intercept]{.subpoint}

[$\beta_1$ = slope for $x$]{.subpoint}


## Regression Example in R

`lm(y ~ x, data = data)`

[○ `y` = outcome]{.subpoint}

[○ `x` = predictor, use `1` for just intercept]{.subpoint}

[○ `data` = dataframe that includes `x` and `y`]{.subpoint}

`summary(model)` to see results


## Regression Example: `penguins`

Let's predict penguins' body mass!

:::: {.columns}
::: {.column width="50%"}
::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
library(palmerpenguins)

p <- penguins |>
  ggplot(aes(y = body_mass_g, x = 0)) +
  geom_jitter() +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(-0.4, 0.6)) +
  guides(y = guide_axis(cap = "both"), x = "none") +
  labs(x = NULL) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
p
```

### Code
```{r}
#| eval: false
library(palmerpenguins)
set.seed(123)
p <- penguins |>
  ggplot(aes(y = body_mass_g, x = 0)) +
  geom_jitter() +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(-0.4, 0.6)) +
  guides(y = guide_axis(cap = "both"), x = "none") +
  labs(x = NULL) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
p
```
:::
:::

::: {.column .fragment width="50%"}
With just this one variable, what would be our best prediction for these data? 
:::
::::


## Regression Example in R: Intercept only

::: fragment
```{r}
#| column: screen
penguins_m0 <- lm(body_mass_g ~ 1, data = penguins)
summary(penguins_m0)
```
:::


## Regression Example in R: Predictions

We can extract predicted values to create our line of best fit with `predict(model, newdata)`

<div class = "subpoint">○ `newdata` = dataframe containing theoretical values of the predictor(s)]</div>

<div class="subsubpoint">○ Should have the same column name(s)</div>

<div class="subsubpoint">○ Don't need for intercept-only</div>

```{r}
predict(penguins_m0) |>
  head(16)
```

:::: {.columns style="font-size:90%"}
::: {.column width="33%"}
```{r}
#| code-line-numbers: false
penguins_m0 |> predict() |>
  length()
```
:::

::: {.column width="33%"}
```{r}
#| code-line-numbers: false
penguins |> select(body_mass_g) |>
  nrow()
```
:::

::: {.column width="33%"}
```{r}
#| code-line-numbers: false
penguins |> select(body_mass_g) |>
  drop_na() |> nrow()
```
:::
::::


## Regression Example: `penguins`

Let's predict penguins' body mass!

:::: {.columns}
::: {.column width="50%"}
::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
library(palmerpenguins)

p <- penguins |>
  ggplot(aes(y = body_mass_g, x = 0)) +
  geom_jitter() +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(-0.4, 0.6)) +
  guides(y = guide_axis(cap = "both"), x = "none") +
  labs(x = NULL) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
p
```

### Code
```{r}
#| eval: false
library(palmerpenguins)
set.seed(123)
p <- penguins |>
  ggplot(aes(y = body_mass_g, x = 0)) +
  geom_jitter() +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(-0.4, 0.6)) +
  guides(y = guide_axis(cap = "both"), x = "none") +
  labs(x = NULL) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
p
```
:::
:::

::: {.column .fragment width="50%"}
With just this one variable, what would be our best prediction for these data? [The mean!]{.fragment}
:::
::::


## Regression Example: `penguins`

Let's predict penguins' body mass!

:::: {.columns}
::: {.column width="50%"}
::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
set.seed(123)
p + geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5) +
  stat_summary(aes(x = .52, y = mean(body_mass_g, na.rm=T),
                   label = mean(body_mass_g, na.rm=T) |> round(1)), geom = "text",
               fun = mean, vjust = -.45, color = "seagreen3", size = 6)
```

### Code
```{r}
#| eval: false
p + geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5) +
  stat_summary(aes(x = .52, y = mean(body_mass_g, na.rm=T),
                   label = mean(body_mass_g, na.rm=T) |> round(1)), geom = "text",
               fun = mean, vjust = -.45, color = "seagreen3", size = 6)
```
:::
:::

::: {.column width="50%"}
With just this one variable, what would be our best prediction for these data? The mean!

[If we needed to predict any random penguin's body mass with only these data, our *best estimate* would be the mean, `r mean(penguins$body_mass_g, na.rm=T) |> round(1)`.]{.fragment}

[Now, let's take into account another variable: a penguin's flipper length]{.fragment}
:::
::::


## Regression Example: `penguins`

:::: {.columns}
::: {.column width="50%"}
::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
penguins |>
  ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +
  geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5) +
  geom_point() +
  scale_x_continuous(breaks = c(170, 210, 250)) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(170, 250), clip = "off") +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
```

### Code
```{r}
#| eval: false
penguins |>
  ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +
  geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5) +
  geom_point() +
  scale_x_continuous(breaks = c(170, 210, 250)) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(170, 250), clip = "off") +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
```
:::
:::

::: {.column width="50%"}
Now, let's take into account another variable: a penguin's flipper length

[Just the mean no longer seems like our line of best fit]{.fragment}
:::
::::




## Regression Example in R: Adding a predictor

::: fragment
```{r}
penguins_m1 <- lm(body_mass_g ~ flipper_length_mm, data = penguins)
summary(penguins_m1)
```
:::


## Regression Example in R: Output anatomy

::: panel-tabset
### `summary()`
```{r}
summary(penguins_m1)
```

### Coefficients
```{r}
summary(penguins_m1)$coefficients
```

:::: {.columns style="font-size:70%"}
::: {.column width="55%"}
**`Estimtate`**:

[○ `(Intercept)`: tells us the estimated outcome value when all predictors are zero.]{.subpoint}

[○ Predictors: tells us the estimated change in the outcome for one unit increase in that predictor]{.subpoint}
:::

::: {.column width="15%"}
**`Std. Error`**: estimated standard error ($SE$) for that parameter
:::

::: {.column width="15%"}
**`t value`**: *t*-statistic for a test of the null that the parameter is equal to zero
:::

::: {.column width="15%"}
**Pr(>|t|)**: *p*-value of that *t*-test
:::
::::

### $R^2$
```{r}
summary(penguins_m1)$r.squared
```

**`Multiple R-squared` aka `r.squared`**: Proportion of variance the model is accounting for (relative to the total variance)

```{r}
summary(penguins_m1)$adj.r.squared
```

**`Adjusted R-squared` aka `adj.r.squared`**: Like `r.squared`, but penalizes for number of predictors

### F-Statistic
```{r}
summary(penguins_m1)$fstatistic
```

**`F-statistic` aka `fstatistic`**: Tells us whether the model accounts for a statistically significant amount of variance (more on this next week!)

:::


## Regression Example in R: Line of best fit

We can extract predicted values to create our line of best fit with `predict(model, newdata)`

[○ `newdata` = dataframe containing theoretical values of the predictor(s)]{.subpoint}

<div class="subsubpoint">○ Should have the same column name(s)</div>

:::: {.columns}
::: {.column width="50%"}
```{r}
penguins |> select(flipper_length_mm) |> range(na.rm = T)

newdata <- data.frame(flipper_length_mm = 170:240)
head(newdata, 50)
```
:::

::: {.column width="50%"}
```{r}
predict(penguins_m1, newdata = newdata) |>
  head(32)
```
:::
::::


## Regression Example in R: Line of best fit

```{r}
predicted_data <- data.frame(flipper_length_mm = seq(170, 240, 1)) |>
  mutate(predicted_body_mass = predict(penguins_m1, newdata = data.frame(flipper_length_mm)))
```

::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
penguins |>
  ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +
  geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5, alpha = .33,
             linetype = "dashed") +
  geom_point() +
    geom_line(data = predicted_data,
            aes(x = flipper_length_mm, y = predicted_body_mass),
            color = "seagreen3", linewidth = 1.5) +
  scale_x_continuous(breaks = c(170, 210, 250)) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(170, 250), clip = "off") +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
```

### Code
```{r}
#| eval: false
penguins |>
  ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +
  geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5, alpha = .33,
             linetype = "dashed") +
  geom_point() +
    geom_line(data = predicted_data,
            aes(x = flipper_length_mm, y = predicted_body_mass),
            color = "seagreen3", linewidth = 1.5) +
  scale_x_continuous(breaks = c(170, 210, 250)) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(170, 250), clip = "off") +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
```
:::


## Regression Example in R: More predictors

We can add predictors to the right hand side of the formula with `+`.

```{r}
penguins_m2 <- lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = penguins)
summary(penguins_m2)
```


## Regression Example in R: More predictors

Can get confidence intervals around predictions with `confint(model)`

```{r}
confint(penguins_m2)
```


## Regression Example in R: More predictions

```{r}
predicted_data_fl <- data.frame(flipper_length_mm = 170:240,
                                bill_length_mm = mean(penguins$bill_length_mm, na.rm = T)) |>
  mutate(predicted_body_mass = predict(penguins_m2, newdata = data.frame(flipper_length_mm,
                                                                         bill_length_mm)))

predicted_data_bl <- data.frame(flipper_length_mm = mean(penguins$flipper_length_mm, na.rm = T),
                                bill_length_mm = 30:60) |>
  mutate(predicted_body_mass = predict(penguins_m2, newdata = data.frame(flipper_length_mm,
                                                                         bill_length_mm)))
```

::: {.panel-tabset}
### Plot
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| fig-align: center

penguins |>
  ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +
  geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5, alpha = .33,
             linetype = "dashed") +
  geom_point() +
    geom_line(data = predicted_data_fl,
            aes(x = flipper_length_mm, y = predicted_body_mass),
            color = "seagreen3", linewidth = 1.5) +
  scale_x_continuous(breaks = c(170, 210, 250)) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(170, 250), clip = "off") +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)

penguins |>
  ggplot(aes(y = body_mass_g, x = bill_length_mm)) +
  geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5, alpha = .33,
             linetype = "dashed") +
  geom_point() +
    geom_line(data = predicted_data_bl,
            aes(x = bill_length_mm, y = predicted_body_mass),
            color = "seagreen3", linewidth = 1.5) +
  scale_x_continuous(breaks = c(30, 45, 60)) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(30, 60), clip = "off") +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
```

### Code
```{r}
#| eval: false
penguins |>
  ggplot(aes(y = body_mass_g, x = flipper_length_mm)) +
  geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5, alpha = .33,
             linetype = "dashed") +
  geom_point() +
    geom_line(data = predicted_data_fl,
            aes(x = flipper_length_mm, y = predicted_body_mass),
            color = "seagreen3", linewidth = 1.5) +
  scale_x_continuous(breaks = c(170, 210, 250)) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(170, 250), clip = "off") +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)

penguins |>
  ggplot(aes(y = body_mass_g, x = bill_length_mm)) +
  geom_hline(aes(yintercept = mean(body_mass_g, na.rm=T)),
             color = "seagreen3", linewidth = 1.5, alpha = .33,
             linetype = "dashed") +
  geom_point() +
    geom_line(data = predicted_data_bl,
            aes(x = bill_length_mm, y = predicted_body_mass),
            color = "seagreen3", linewidth = 1.5) +
  scale_x_continuous(breaks = c(30, 45, 60)) +
  scale_y_continuous(breaks = c(2000, 4500, 7000)) +
  coord_cartesian(ylim = c(2000, 7000), xlim = c(30, 60), clip = "off") +
  guides(y = guide_axis(cap = "both"), x = guide_axis(cap = "both")) +
  theme_classic(base_size = 16) +
  theme(aspect.ratio = 1)
```
:::


## Regression Example: Writeup

Controlling for bill length, the effect of flipper length on penguins body mass was significant (*p* < .001). For every mm increase in flipper length, we expect to see a 46.15g increase in body mass. Controlling for flipper length, the effect of bill length on penguins' body mass was not significant (*p* = .244). The expected body mass when flipper length and bill length are both zero is -5736.897.

::: callout-note
Why is the intercept so crazy? [Because flipper length and bill length would never actually be zero!]{.fragment}
:::


# Assignment 13
















